{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbbe4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(\"wind_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d37813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TurbID  Day Tmstamp  Wspd  Wdir   Etmp   Itmp   Ndir  Pab1  Pab2  Pab3  \\\n",
      "0       1    1   00:00   NaN   NaN    NaN    NaN    NaN   NaN   NaN   NaN   \n",
      "1       1    1   00:10  6.17 -3.99  30.73  41.80  25.92   1.0   1.0   1.0   \n",
      "2       1    1   00:20  6.27 -2.18  30.60  41.63  20.91   1.0   1.0   1.0   \n",
      "3       1    1   00:30  6.42 -0.73  30.52  41.52  20.91   1.0   1.0   1.0   \n",
      "4       1    1   00:40  6.25  0.89  30.49  41.38  20.91   1.0   1.0   1.0   \n",
      "\n",
      "   Prtv    Patv  \n",
      "0   NaN     NaN  \n",
      "1 -0.25  494.66  \n",
      "2 -0.24  509.76  \n",
      "3 -0.26  542.53  \n",
      "4 -0.23  509.36  \n",
      "(10000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a35c18db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TurbID       0\n",
       "Day          0\n",
       "Tmstamp      0\n",
       "Wspd       241\n",
       "Wdir       241\n",
       "Etmp       241\n",
       "Itmp       241\n",
       "Ndir       241\n",
       "Pab1       241\n",
       "Pab2       241\n",
       "Pab3       241\n",
       "Prtv       241\n",
       "Patv       241\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b28359f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TurbID     0\n",
       "Day        0\n",
       "Tmstamp    0\n",
       "Wspd       0\n",
       "Wdir       0\n",
       "Etmp       0\n",
       "Itmp       0\n",
       "Ndir       0\n",
       "Pab1       0\n",
       "Pab2       0\n",
       "Pab3       0\n",
       "Prtv       0\n",
       "Patv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520a77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9759, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a19deb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9759 entries, 1 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   TurbID  9759 non-null   int64  \n",
      " 1   Day     9759 non-null   int64  \n",
      " 2   Wspd    9759 non-null   float64\n",
      " 3   Wdir    9759 non-null   float64\n",
      " 4   Etmp    9759 non-null   float64\n",
      " 5   Itmp    9759 non-null   float64\n",
      " 6   Ndir    9759 non-null   float64\n",
      " 7   Pab1    9759 non-null   float64\n",
      " 8   Pab2    9759 non-null   float64\n",
      " 9   Pab3    9759 non-null   float64\n",
      " 10  Prtv    9759 non-null   float64\n",
      " 11  Patv    9759 non-null   float64\n",
      " 12  hour    9759 non-null   int64  \n",
      " 13  minute  9759 non-null   int64  \n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "data[['hour', 'minute']] = data['Tmstamp'].str.split(':', expand=True)\n",
    "data['hour'] = pd.to_numeric(data['hour'])\n",
    "data['minute'] = pd.to_numeric(data['minute'])\n",
    "data.drop(columns=['Tmstamp'], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52af7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "1          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "2          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "3          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "4          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "5          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   70     9      50  3.22  2.52  29.88  38.09  434.95  0.99  0.99   \n",
      "9996       1   70    10       0  2.43  2.16  30.15  38.34  438.17  0.99  0.99   \n",
      "9997       1   70    10      10  2.54  5.65  30.45  38.69  452.78  0.99  0.99   \n",
      "9998       1   70    10      20  2.94 -6.57  30.69  39.11  465.65  0.99  0.99   \n",
      "9999       1   70    10      30  2.03  4.16  31.01  39.50  466.50  0.99  0.99   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "1     1.00 -0.25  494.66  \n",
      "2     1.00 -0.24  509.76  \n",
      "3     1.00 -0.26  542.53  \n",
      "4     1.00 -0.23  509.36  \n",
      "5     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  0.99 -0.26  127.05  \n",
      "9996  0.99 -0.22   80.31  \n",
      "9997  0.99 -0.25   82.65  \n",
      "9998  0.99 -0.22  100.32  \n",
      "9999  0.99 -0.23   50.60  \n",
      "\n",
      "[9759 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "desired_order = [\n",
    "    'TurbID', 'Day', 'hour', 'minute','Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir',\n",
    "    'Pab1', 'Pab2', 'Pab3', 'Prtv', 'Patv']\n",
    "data = data.reindex(columns=desired_order)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86cb930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "1          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "2          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "3          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "4          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "5          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   70     9      50  3.22  2.52  29.88  38.09  434.95  0.99  0.99   \n",
      "9996       1   70    10       0  2.43  2.16  30.15  38.34  438.17  0.99  0.99   \n",
      "9997       1   70    10      10  2.54  5.65  30.45  38.69  452.78  0.99  0.99   \n",
      "9998       1   70    10      20  2.94 -6.57  30.69  39.11  465.65  0.99  0.99   \n",
      "9999       1   70    10      30  2.03  4.16  31.01  39.50  466.50  0.99  0.99   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "1     1.00 -0.25  494.66  \n",
      "2     1.00 -0.24  509.76  \n",
      "3     1.00 -0.26  542.53  \n",
      "4     1.00 -0.23  509.36  \n",
      "5     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  0.99 -0.26  127.05  \n",
      "9996  0.99 -0.22   80.31  \n",
      "9997  0.99 -0.25   82.65  \n",
      "9998  0.99 -0.22  100.32  \n",
      "9999  0.99 -0.23   50.60  \n",
      "\n",
      "[9759 rows x 14 columns]\n",
      "\n",
      "DataFrame after dropping outliers:\n",
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "1          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "2          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "3          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "4          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "5          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   70     9      50  3.22  2.52  29.88  38.09  434.95  0.99  0.99   \n",
      "9996       1   70    10       0  2.43  2.16  30.15  38.34  438.17  0.99  0.99   \n",
      "9997       1   70    10      10  2.54  5.65  30.45  38.69  452.78  0.99  0.99   \n",
      "9998       1   70    10      20  2.94 -6.57  30.69  39.11  465.65  0.99  0.99   \n",
      "9999       1   70    10      30  2.03  4.16  31.01  39.50  466.50  0.99  0.99   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "1     1.00 -0.25  494.66  \n",
      "2     1.00 -0.24  509.76  \n",
      "3     1.00 -0.26  542.53  \n",
      "4     1.00 -0.23  509.36  \n",
      "5     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  0.99 -0.26  127.05  \n",
      "9996  0.99 -0.22   80.31  \n",
      "9997  0.99 -0.25   82.65  \n",
      "9998  0.99 -0.22  100.32  \n",
      "9999  0.99 -0.23   50.60  \n",
      "\n",
      "[6760 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def drop_outliers_iqr(dataframe):\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    #1.5 is threshold\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = ((dataframe < lower_bound) | (dataframe > upper_bound)).any(axis=1)\n",
    "    cleaned_dataframe = dataframe[~outliers]\n",
    "    return cleaned_dataframe\n",
    "cleaned_df = drop_outliers_iqr(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "print(\"\\nDataFrame after dropping outliers:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8a55fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TurbID  Day      hour  minute      Wspd      Wdir      Etmp      Itmp  \\\n",
      "0        0.0  0.0  0.000000     0.2  0.359792  0.310772  0.611111  0.627168   \n",
      "1        0.0  0.0  0.000000     0.4  0.366298  0.397045  0.606421  0.621026   \n",
      "2        0.0  0.0  0.000000     0.6  0.376057  0.466158  0.603535  0.617052   \n",
      "3        0.0  0.0  0.000000     0.8  0.364997  0.543375  0.602453  0.611994   \n",
      "4        0.0  0.0  0.000000     1.0  0.355237  0.451859  0.601732  0.606214   \n",
      "...      ...  ...       ...     ...       ...       ...       ...       ...   \n",
      "6755     0.0  1.0  0.391304     1.0  0.167859  0.621068  0.580447  0.493136   \n",
      "6756     0.0  1.0  0.434783     0.0  0.116461  0.603908  0.590188  0.502168   \n",
      "6757     0.0  1.0  0.434783     0.2  0.123617  0.770257  0.601010  0.514812   \n",
      "6758     0.0  1.0  0.434783     0.4  0.149642  0.187798  0.609668  0.529986   \n",
      "6759     0.0  1.0  0.434783     0.6  0.090436  0.699237  0.621212  0.544075   \n",
      "\n",
      "          Ndir      Pab1      Pab2      Pab3      Prtv      Patv  \n",
      "0     0.277969  0.010989  0.015385  0.010989  0.500000  0.320070  \n",
      "1     0.272074  0.010989  0.015385  0.010989  0.541667  0.329787  \n",
      "2     0.272074  0.010989  0.015385  0.010989  0.458333  0.350877  \n",
      "3     0.272074  0.010989  0.015385  0.010989  0.583333  0.329530  \n",
      "4     0.272074  0.010989  0.015385  0.010989  0.416667  0.312057  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "6755  0.759248  0.000000  0.000000  0.000000  0.458333  0.083489  \n",
      "6756  0.763037  0.000000  0.000000  0.000000  0.625000  0.053409  \n",
      "6757  0.780228  0.000000  0.000000  0.000000  0.500000  0.054915  \n",
      "6758  0.795371  0.000000  0.000000  0.000000  0.625000  0.066287  \n",
      "6759  0.796371  0.000000  0.000000  0.000000  0.583333  0.034289  \n",
      "\n",
      "[6760 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cleaned_df_normalized = scaler.fit_transform(cleaned_df)\n",
    "cleaned_df_normalized_df = pd.DataFrame(cleaned_df_normalized, columns=cleaned_df.columns)\n",
    "print(cleaned_df_normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c87947bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cleaned_df_normalized_df.drop(columns=['Patv'])\n",
    "y=cleaned_df_normalized_df[\"Patv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "668a7e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Wspd      Etmp      Itmp      Pab1      Pab2      Pab3\n",
      "0     0.359792  0.611111  0.627168  0.010989  0.015385  0.010989\n",
      "1     0.366298  0.606421  0.621026  0.010989  0.015385  0.010989\n",
      "2     0.376057  0.603535  0.617052  0.010989  0.015385  0.010989\n",
      "3     0.364997  0.602453  0.611994  0.010989  0.015385  0.010989\n",
      "4     0.355237  0.601732  0.606214  0.010989  0.015385  0.010989\n",
      "...        ...       ...       ...       ...       ...       ...\n",
      "6755  0.167859  0.580447  0.493136  0.000000  0.000000  0.000000\n",
      "6756  0.116461  0.590188  0.502168  0.000000  0.000000  0.000000\n",
      "6757  0.123617  0.601010  0.514812  0.000000  0.000000  0.000000\n",
      "6758  0.149642  0.609668  0.529986  0.000000  0.000000  0.000000\n",
      "6759  0.090436  0.621212  0.544075  0.000000  0.000000  0.000000\n",
      "\n",
      "[6760 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# wrapper method\n",
    "# Recursive Feature Elimination (RFE)\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=6) \n",
    "rfe = rfe.fit(x, y)\n",
    "# Get selected features\n",
    "selected_features = x.columns[rfe.support_]\n",
    "df_reduced = x[selected_features]\n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35fab150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6084, 6), (676,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_reduced,y,test_size=0.1,random_state=42)\n",
    "x_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36826a7",
   "metadata": {},
   "source": [
    "# vae lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeaf6912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "96/96 [==============================] - 6s 20ms/step - loss: 0.0891 - val_loss: 0.0738\n",
      "Epoch 2/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0743 - val_loss: 0.0692\n",
      "Epoch 3/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0710 - val_loss: 0.0714\n",
      "Epoch 4/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0703 - val_loss: 0.0726\n",
      "Epoch 5/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0695 - val_loss: 0.0696\n",
      "Epoch 6/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0705 - val_loss: 0.0687\n",
      "Epoch 7/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0705 - val_loss: 0.0692\n",
      "Epoch 8/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0703 - val_loss: 0.0679\n",
      "Epoch 9/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0684 - val_loss: 0.0750\n",
      "Epoch 10/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0721 - val_loss: 0.0813\n",
      "Epoch 11/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0697 - val_loss: 0.0718\n",
      "Epoch 12/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0663 - val_loss: 0.0722\n",
      "Epoch 13/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0667 - val_loss: 0.0677\n",
      "Epoch 14/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0687 - val_loss: 0.0680\n",
      "Epoch 15/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0666 - val_loss: 0.0714\n",
      "Epoch 16/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0679 - val_loss: 0.0700\n",
      "Epoch 17/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0683 - val_loss: 0.0694\n",
      "Epoch 18/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0662 - val_loss: 0.0751\n",
      "Epoch 19/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0689 - val_loss: 0.0684\n",
      "Epoch 20/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0690 - val_loss: 0.0695\n",
      "Epoch 21/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0675 - val_loss: 0.0681\n",
      "Epoch 22/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0666 - val_loss: 0.0689\n",
      "Epoch 23/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0667 - val_loss: 0.0694\n",
      "Epoch 24/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0680 - val_loss: 0.0702\n",
      "Epoch 25/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0673 - val_loss: 0.0712\n",
      "Epoch 26/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0686 - val_loss: 0.0692\n",
      "Epoch 27/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0679 - val_loss: 0.0691\n",
      "Epoch 28/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0671 - val_loss: 0.0702\n",
      "Epoch 29/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0651 - val_loss: 0.0704\n",
      "Epoch 30/200\n",
      "96/96 [==============================] - 2s 17ms/step - loss: 0.0687 - val_loss: 0.0761\n",
      "Epoch 31/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0669 - val_loss: 0.0689\n",
      "Epoch 32/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0665 - val_loss: 0.0686\n",
      "Epoch 33/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0664 - val_loss: 0.0662\n",
      "Epoch 34/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0665 - val_loss: 0.0692\n",
      "Epoch 35/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0652 - val_loss: 0.0677\n",
      "Epoch 36/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0693 - val_loss: 0.0705\n",
      "Epoch 37/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0681 - val_loss: 0.0672\n",
      "Epoch 38/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0648 - val_loss: 0.0680\n",
      "Epoch 39/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0665 - val_loss: 0.0686\n",
      "Epoch 40/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0668 - val_loss: 0.0688\n",
      "Epoch 41/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0655 - val_loss: 0.0687\n",
      "Epoch 42/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0656 - val_loss: 0.0699\n",
      "Epoch 43/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0671 - val_loss: 0.0680\n",
      "Epoch 44/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0657 - val_loss: 0.0761\n",
      "Epoch 45/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0679 - val_loss: 0.0670\n",
      "Epoch 46/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0679 - val_loss: 0.0707\n",
      "Epoch 47/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0656 - val_loss: 0.0691\n",
      "Epoch 48/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0633 - val_loss: 0.0690\n",
      "Epoch 49/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0671 - val_loss: 0.0695\n",
      "Epoch 50/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0635 - val_loss: 0.0701\n",
      "Epoch 51/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0667 - val_loss: 0.0743\n",
      "Epoch 52/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0658 - val_loss: 0.0723\n",
      "Epoch 53/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0672 - val_loss: 0.0697\n",
      "Epoch 54/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0639 - val_loss: 0.0705\n",
      "Epoch 55/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0654 - val_loss: 0.0689\n",
      "Epoch 56/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0659 - val_loss: 0.0759\n",
      "Epoch 57/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0654 - val_loss: 0.0759\n",
      "Epoch 58/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0691 - val_loss: 0.0705\n",
      "Epoch 59/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0664 - val_loss: 0.0673\n",
      "Epoch 60/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0658 - val_loss: 0.0684\n",
      "Epoch 61/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0657 - val_loss: 0.0712\n",
      "Epoch 62/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0686 - val_loss: 0.0691\n",
      "Epoch 63/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0631 - val_loss: 0.0694\n",
      "Epoch 64/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0642 - val_loss: 0.0706\n",
      "Epoch 65/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0625 - val_loss: 0.0690\n",
      "Epoch 66/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0685 - val_loss: 0.0661\n",
      "Epoch 67/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0669 - val_loss: 0.0705\n",
      "Epoch 68/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0670 - val_loss: 0.0676\n",
      "Epoch 69/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0638 - val_loss: 0.0687\n",
      "Epoch 70/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0665 - val_loss: 0.0703\n",
      "Epoch 71/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0651 - val_loss: 0.0716\n",
      "Epoch 72/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0640 - val_loss: 0.0702\n",
      "Epoch 73/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0655 - val_loss: 0.0707\n",
      "Epoch 74/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0674 - val_loss: 0.0685\n",
      "Epoch 75/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0657 - val_loss: 0.0687\n",
      "Epoch 76/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0644 - val_loss: 0.0704\n",
      "Epoch 77/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0657 - val_loss: 0.0674\n",
      "Epoch 78/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0674 - val_loss: 0.0676\n",
      "Epoch 79/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0664 - val_loss: 0.0678\n",
      "Epoch 80/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0668 - val_loss: 0.0679\n",
      "Epoch 81/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0650 - val_loss: 0.0681\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0645 - val_loss: 0.0700\n",
      "Epoch 83/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0651 - val_loss: 0.0697\n",
      "Epoch 84/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0670 - val_loss: 0.0676\n",
      "Epoch 85/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0674 - val_loss: 0.0691\n",
      "Epoch 86/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0657 - val_loss: 0.0691\n",
      "Epoch 87/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0639 - val_loss: 0.0700\n",
      "Epoch 88/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0666 - val_loss: 0.0694\n",
      "Epoch 89/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0662 - val_loss: 0.0694\n",
      "Epoch 90/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0643 - val_loss: 0.0708\n",
      "Epoch 91/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0654 - val_loss: 0.0704\n",
      "Epoch 92/200\n",
      "96/96 [==============================] - 1s 14ms/step - loss: 0.0655 - val_loss: 0.0707\n",
      "Epoch 93/200\n",
      "96/96 [==============================] - 1s 13ms/step - loss: 0.0651 - val_loss: 0.0683\n",
      "Epoch 94/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0663 - val_loss: 0.0691\n",
      "Epoch 95/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0669 - val_loss: 0.0688\n",
      "Epoch 96/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0675 - val_loss: 0.0684\n",
      "Epoch 97/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0651 - val_loss: 0.0707\n",
      "Epoch 98/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0669 - val_loss: 0.0786\n",
      "Epoch 99/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0640 - val_loss: 0.0680\n",
      "Epoch 100/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0646 - val_loss: 0.0713\n",
      "Epoch 101/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0653 - val_loss: 0.0699\n",
      "Epoch 102/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0655 - val_loss: 0.0694\n",
      "Epoch 103/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0654 - val_loss: 0.0684\n",
      "Epoch 104/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0657 - val_loss: 0.0705\n",
      "Epoch 105/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0669 - val_loss: 0.0687\n",
      "Epoch 106/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0651 - val_loss: 0.0690\n",
      "Epoch 107/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0666 - val_loss: 0.0705\n",
      "Epoch 108/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0659 - val_loss: 0.0694\n",
      "Epoch 109/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0652 - val_loss: 0.0688\n",
      "Epoch 110/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0654 - val_loss: 0.0712\n",
      "Epoch 111/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0661 - val_loss: 0.0687\n",
      "Epoch 112/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0650 - val_loss: 0.0709\n",
      "Epoch 113/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0646 - val_loss: 0.0718\n",
      "Epoch 114/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0648 - val_loss: 0.0759\n",
      "Epoch 115/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0639 - val_loss: 0.0748\n",
      "Epoch 116/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0713 - val_loss: 0.0680\n",
      "Epoch 117/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0651 - val_loss: 0.0705\n",
      "Epoch 118/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0672 - val_loss: 0.0705\n",
      "Epoch 119/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0665 - val_loss: 0.0684\n",
      "Epoch 120/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0647 - val_loss: 0.0711\n",
      "Epoch 121/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0640 - val_loss: 0.0744\n",
      "Epoch 122/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0668 - val_loss: 0.0671\n",
      "Epoch 123/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0643 - val_loss: 0.0695\n",
      "Epoch 124/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0643 - val_loss: 0.0710\n",
      "Epoch 125/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0633 - val_loss: 0.0709\n",
      "Epoch 126/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0654 - val_loss: 0.0694\n",
      "Epoch 127/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0673 - val_loss: 0.0679\n",
      "Epoch 128/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0659 - val_loss: 0.0686\n",
      "Epoch 129/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0654 - val_loss: 0.0687\n",
      "Epoch 130/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0650 - val_loss: 0.0697\n",
      "Epoch 131/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0671 - val_loss: 0.0685\n",
      "Epoch 132/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0662 - val_loss: 0.0689\n",
      "Epoch 133/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0630 - val_loss: 0.0703\n",
      "Epoch 134/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0643 - val_loss: 0.0683\n",
      "Epoch 135/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0665 - val_loss: 0.0687\n",
      "Epoch 136/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0660 - val_loss: 0.0684\n",
      "Epoch 137/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0649 - val_loss: 0.0696\n",
      "Epoch 138/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0655 - val_loss: 0.0693\n",
      "Epoch 139/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0643 - val_loss: 0.0684\n",
      "Epoch 140/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0652 - val_loss: 0.0705\n",
      "Epoch 141/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0651 - val_loss: 0.0681\n",
      "Epoch 142/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0641 - val_loss: 0.0698\n",
      "Epoch 143/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0658 - val_loss: 0.0694\n",
      "Epoch 144/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0664 - val_loss: 0.0677\n",
      "Epoch 145/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0655 - val_loss: 0.0698\n",
      "Epoch 146/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0654 - val_loss: 0.0671\n",
      "Epoch 147/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0662 - val_loss: 0.0700\n",
      "Epoch 148/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0643 - val_loss: 0.0689\n",
      "Epoch 149/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0642 - val_loss: 0.0690\n",
      "Epoch 150/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0648 - val_loss: 0.0710\n",
      "Epoch 151/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0658 - val_loss: 0.0702\n",
      "Epoch 152/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0674 - val_loss: 0.0688\n",
      "Epoch 153/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0630 - val_loss: 0.0690\n",
      "Epoch 154/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0632 - val_loss: 0.0694\n",
      "Epoch 155/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0659 - val_loss: 0.0709\n",
      "Epoch 156/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0633 - val_loss: 0.0745\n",
      "Epoch 157/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0700 - val_loss: 0.0691\n",
      "Epoch 158/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0673 - val_loss: 0.0672\n",
      "Epoch 159/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0653 - val_loss: 0.0698\n",
      "Epoch 160/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0653 - val_loss: 0.0686\n",
      "Epoch 161/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0653 - val_loss: 0.0682\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0659 - val_loss: 0.0689\n",
      "Epoch 163/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0656 - val_loss: 0.0670\n",
      "Epoch 164/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0653 - val_loss: 0.0691\n",
      "Epoch 165/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0632 - val_loss: 0.0697\n",
      "Epoch 166/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0632 - val_loss: 0.0691\n",
      "Epoch 167/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0612 - val_loss: 0.0739\n",
      "Epoch 168/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0656 - val_loss: 0.0687\n",
      "Epoch 169/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0644 - val_loss: 0.0681\n",
      "Epoch 170/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0669 - val_loss: 0.0661\n",
      "Epoch 171/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0662 - val_loss: 0.0667\n",
      "Epoch 172/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0655 - val_loss: 0.0691\n",
      "Epoch 173/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0654 - val_loss: 0.0685\n",
      "Epoch 174/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0660 - val_loss: 0.0685\n",
      "Epoch 175/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0639 - val_loss: 0.0696\n",
      "Epoch 176/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0650 - val_loss: 0.0698\n",
      "Epoch 177/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0675 - val_loss: 0.0665\n",
      "Epoch 178/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0661 - val_loss: 0.0669\n",
      "Epoch 179/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0668 - val_loss: 0.0677\n",
      "Epoch 180/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0663 - val_loss: 0.0666\n",
      "Epoch 181/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0659 - val_loss: 0.0679\n",
      "Epoch 182/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0661 - val_loss: 0.0686\n",
      "Epoch 183/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0642 - val_loss: 0.0691\n",
      "Epoch 184/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0609 - val_loss: 0.0725\n",
      "Epoch 185/200\n",
      "96/96 [==============================] - 1s 16ms/step - loss: 0.0674 - val_loss: 0.0683\n",
      "Epoch 186/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0672 - val_loss: 0.0672\n",
      "Epoch 187/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0627 - val_loss: 0.0712\n",
      "Epoch 188/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0644 - val_loss: 0.0699\n",
      "Epoch 189/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0675 - val_loss: 0.0672\n",
      "Epoch 190/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0665 - val_loss: 0.0683\n",
      "Epoch 191/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0632 - val_loss: 0.0727\n",
      "Epoch 192/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0639 - val_loss: 0.0687\n",
      "Epoch 193/200\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 0.0661 - val_loss: 0.0687\n",
      "Epoch 194/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0673 - val_loss: 0.0752\n",
      "Epoch 195/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0659 - val_loss: 0.0685\n",
      "Epoch 196/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0671 - val_loss: 0.0685\n",
      "Epoch 197/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0682 - val_loss: 0.0686\n",
      "Epoch 198/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0664 - val_loss: 0.0682\n",
      "Epoch 199/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0639 - val_loss: 0.0681\n",
      "Epoch 200/200\n",
      "96/96 [==============================] - 1s 15ms/step - loss: 0.0644 - val_loss: 0.0694\n",
      "191/191 [==============================] - 1s 3ms/step\n",
      "191/191 [==============================] - 1s 2ms/step\n",
      "Sequence Length: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models \n",
    "import numpy as np\n",
    "latent_dim = 6\n",
    "input_shape = 6\n",
    "conv_input_shape = (input_shape, 1)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=conv_input_shape)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(encoder_inputs)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Sampler\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(decoder_inputs)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(input_shape * 32, activation=tf.nn.leaky_relu)(x)  # To match the flattened Conv1D output\n",
    "x = layers.Reshape((input_shape, 32))(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(x)\n",
    "decoder_outputs = layers.Conv1D(1, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
    "decoder_outputs = layers.Flatten()(decoder_outputs)\n",
    "\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
    "\n",
    "# Loss\n",
    "def vae_loss(x, outputs):\n",
    "    x_decoded_mean = outputs\n",
    "    z_log_var = outputs[1]\n",
    "    z_mean = outputs[0]\n",
    "    \n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(x, x_decoded_mean), axis=-1)\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "\n",
    "vae.fit(x_train, x_train, epochs=200, batch_size=64, validation_data=(x_test, x_test))\n",
    "\n",
    "latent_train = encoder.predict(x_train)[2]  # Extracting only the z vector\n",
    "reconstructed_data = decoder.predict(latent_train)\n",
    "latent_train_array = np.array(latent_train)\n",
    "sequence_length = latent_train_array.shape[1]\n",
    "print(\"Sequence Length:\", sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7974c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 4ms/step\n",
      "Mean Reconstruction Error for test data: 0.0262\n",
      "191/191 [==============================] - 1s 5ms/step\n",
      "Mean Reconstruction Error for train data: 0.0262\n"
     ]
    }
   ],
   "source": [
    "reconstructed_test = vae.predict(x_test)\n",
    "reconstruction_errors_test = np.mean(np.square(x_test - reconstructed_test), axis=1)\n",
    "print(f\"Mean Reconstruction Error for test data: {np.mean(reconstruction_errors_test):.4f}\")\n",
    "reconstructed_train = vae.predict(x_train)\n",
    "reconstruction_errors_train = np.mean(np.square(x_train - reconstructed_train), axis=1)\n",
    "print(f\"Mean Reconstruction Error for train data: {np.mean(reconstruction_errors_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a019f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.026237435194573593\n",
      "Root Mean Squared Error (RMSE): 0.1619797369876047\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "original_data_array = x_train.values\n",
    "original_data_flat = original_data_array.flatten()\n",
    "reconstructed_data_flat = reconstructed_data.flatten()\n",
    "mse = mean_squared_error(original_data_flat, reconstructed_data_flat)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "740f208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6084, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "latent_train_reshaped = latent_train_array.reshape(latent_train_array.shape[0], latent_train_array.shape[1],1)\n",
    "print(latent_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27c27418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step\n",
      "(676, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "latent_test = encoder.predict(x_test)[2]\n",
    "latent_test_array = np.array(latent_test)\n",
    "latent_test_reshaped = latent_test_array.reshape(latent_test_array.shape[0], latent_test_array.shape[1],1)\n",
    "print(latent_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "897736dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "761/761 [==============================] - 23s 23ms/step - loss: 0.0736\n",
      "Epoch 2/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0077\n",
      "Epoch 3/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0049\n",
      "Epoch 4/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0041\n",
      "Epoch 5/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0036\n",
      "Epoch 6/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0031\n",
      "Epoch 7/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0030\n",
      "Epoch 8/10\n",
      "761/761 [==============================] - 18s 23ms/step - loss: 0.0028\n",
      "Epoch 9/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0026\n",
      "Epoch 10/10\n",
      "761/761 [==============================] - 17s 23ms/step - loss: 0.0024\n",
      "22/22 [==============================] - 1s 10ms/step - loss: 0.0015\n",
      "Test Loss: 0.0014832207234576344\n",
      "22/22 [==============================] - 1s 10ms/step\n",
      "Mean Squared Error (MSE): 0.0014832208704826217\n",
      "Mean Absolute Error (MAE): 0.028059983891555507\n",
      "R-squared (R2) Score: 0.9856640653193189\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Dropout\n",
    "vae_lstm = Sequential()\n",
    "vae_lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6,1)))\n",
    "vae_lstm.add(Dropout(0.2))\n",
    "vae_lstm.add(LSTM(units=150, activation='relu'))\n",
    "vae_lstm.add(Dense(units=1)) \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "vae_lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "vae_lstm.fit(latent_train_reshaped, y_train, epochs=10, batch_size=8)\n",
    "loss = vae_lstm.evaluate(latent_test_reshaped, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = vae_lstm.predict(latent_test_reshaped)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf89f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_lstm.save('vae_lstm.h5')\n",
    "# from tensorflow.keras.models import load_model\n",
    "# vae_lstm = load_model('vae_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7769e",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5725e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6084, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "feature=1\n",
    "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],feature))\n",
    "print(x_train.shape)\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values\n",
    "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c242d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "761/761 [==============================] - 20s 22ms/step - loss: 0.0894\n",
      "Epoch 2/10\n",
      "761/761 [==============================] - 16s 22ms/step - loss: 0.0357\n",
      "Epoch 3/10\n",
      "761/761 [==============================] - 16s 22ms/step - loss: 0.0139\n",
      "Epoch 4/10\n",
      "761/761 [==============================] - 16s 22ms/step - loss: 0.0098\n",
      "Epoch 5/10\n",
      "761/761 [==============================] - 17s 22ms/step - loss: 0.0081\n",
      "Epoch 6/10\n",
      "761/761 [==============================] - 17s 22ms/step - loss: 0.0072\n",
      "Epoch 7/10\n",
      "761/761 [==============================] - 16s 22ms/step - loss: 0.0063\n",
      "Epoch 8/10\n",
      "761/761 [==============================] - 17s 22ms/step - loss: 0.0055\n",
      "Epoch 9/10\n",
      "761/761 [==============================] - 17s 22ms/step - loss: 0.0045\n",
      "Epoch 10/10\n",
      "761/761 [==============================] - 17s 22ms/step - loss: 0.0039\n",
      "22/22 [==============================] - 1s 10ms/step - loss: 0.0027\n",
      "Test Loss: 0.0027434860821813345\n",
      "22/22 [==============================] - 1s 10ms/step\n",
      "Mean Squared Error (MSE): 0.0027434861954387228\n",
      "Mean Absolute Error (MAE): 0.0389283589655289\n",
      "R-squared (R2) Score: 0.9734830869239576\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6,1)))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(units=150, activation='relu'))\n",
    "lstm.add(Dense(units=1))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "lstm.fit(x_train, y_train, epochs=10, batch_size=8)\n",
    "loss = lstm.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = lstm.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6001b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vigne\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "lstm.save('lstm.h5')\n",
    "# from tensorflow.keras.models import load_model\n",
    "# lstm = load_model('lstm.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
