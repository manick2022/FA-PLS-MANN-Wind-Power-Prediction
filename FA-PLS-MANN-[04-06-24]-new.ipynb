{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(\"wind_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19deb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['hour', 'minute']] = data['Tmstamp'].str.split(':', expand=True)\n",
    "data['hour'] = pd.to_numeric(data['hour'])\n",
    "data['minute'] = pd.to_numeric(data['minute'])\n",
    "data.drop(columns=['Tmstamp'], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_order = [\n",
    "    'TurbID', 'Day', 'hour', 'minute','Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir',\n",
    "    'Pab1', 'Pab2', 'Pab3', 'Prtv', 'Patv']\n",
    "data = data.reindex(columns=desired_order)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def drop_outliers_iqr(dataframe):\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    #1.5 is threshold\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = ((dataframe < lower_bound) | (dataframe > upper_bound)).any(axis=1)\n",
    "    cleaned_dataframe = dataframe[~outliers]\n",
    "    return cleaned_dataframe\n",
    "cleaned_df = drop_outliers_iqr(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "print(\"\\nDataFrame after dropping outliers:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a55fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cleaned_df_normalized = scaler.fit_transform(cleaned_df)\n",
    "cleaned_df_normalized_df = pd.DataFrame(cleaned_df_normalized, columns=cleaned_df.columns)\n",
    "print(cleaned_df_normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87947bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cleaned_df_normalized_df.drop(columns=['Patv'])\n",
    "y=cleaned_df_normalized_df[\"Patv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper method\n",
    "# Recursive Feature Elimination (RFE)\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=6) \n",
    "rfe = rfe.fit(x, y)\n",
    "# Get selected features\n",
    "selected_features = x.columns[rfe.support_]\n",
    "df_reduced = x[selected_features]\n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fab150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_reduced,y,test_size=0.1,random_state=42)\n",
    "x_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36826a7",
   "metadata": {},
   "source": [
    "# vae lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models \n",
    "import numpy as np\n",
    "latent_dim = 6\n",
    "input_shape = 6\n",
    "conv_input_shape = (input_shape, 1)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=conv_input_shape)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(encoder_inputs)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Sampler\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(decoder_inputs)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(input_shape * 32, activation=tf.nn.leaky_relu)(x)  # To match the flattened Conv1D output\n",
    "x = layers.Reshape((input_shape, 32))(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(x)\n",
    "decoder_outputs = layers.Conv1D(1, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
    "decoder_outputs = layers.Flatten()(decoder_outputs)\n",
    "\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
    "\n",
    "# Loss\n",
    "def vae_loss(x, outputs):\n",
    "    x_decoded_mean = outputs\n",
    "    z_log_var = outputs[1]\n",
    "    z_mean = outputs[0]\n",
    "    \n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(x, x_decoded_mean), axis=-1)\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "\n",
    "vae.fit(x_train, x_train, epochs=200, batch_size=64, validation_data=(x_test, x_test))\n",
    "\n",
    "latent_train = encoder.predict(x_train)[2]  # Extracting only the z vector\n",
    "reconstructed_data = decoder.predict(latent_train)\n",
    "latent_train_array = np.array(latent_train)\n",
    "sequence_length = latent_train_array.shape[1]\n",
    "print(\"Sequence Length:\", sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_test = vae.predict(x_test)\n",
    "reconstruction_errors_test = np.mean(np.square(x_test - reconstructed_test), axis=1)\n",
    "print(f\"Mean Reconstruction Error for test data: {np.mean(reconstruction_errors_test):.4f}\")\n",
    "reconstructed_train = vae.predict(x_train)\n",
    "reconstruction_errors_train = np.mean(np.square(x_train - reconstructed_train), axis=1)\n",
    "print(f\"Mean Reconstruction Error for train data: {np.mean(reconstruction_errors_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "original_data_array = x_train.values\n",
    "original_data_flat = original_data_array.flatten()\n",
    "reconstructed_data_flat = reconstructed_data.flatten()\n",
    "mse = mean_squared_error(original_data_flat, reconstructed_data_flat)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train_reshaped = latent_train_array.reshape(latent_train_array.shape[0], latent_train_array.shape[1],1)\n",
    "print(latent_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c27418",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_test = encoder.predict(x_test)[2]\n",
    "latent_test_array = np.array(latent_test)\n",
    "latent_test_reshaped = latent_test_array.reshape(latent_test_array.shape[0], latent_test_array.shape[1],1)\n",
    "print(latent_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897736dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare data\n",
    "latent_data = np.concatenate((latent_train_reshaped, latent_test_reshaped), axis=0)\n",
    "target_data = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(latent_data):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    x_train_fold, x_test_fold = latent_data[train_index], latent_data[test_index]\n",
    "    y_train_fold, y_test_fold = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    # Define LSTM model\n",
    "    vae_lstm = Sequential()\n",
    "    vae_lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6, 1)))\n",
    "    vae_lstm.add(Dropout(0.2))\n",
    "    vae_lstm.add(LSTM(units=150, activation='relu'))\n",
    "    vae_lstm.add(Dense(units=1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    vae_lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Train model\n",
    "    vae_lstm.fit(x_train_fold, y_train_fold, epochs=10, batch_size=8, verbose=1)\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss = vae_lstm.evaluate(x_test_fold, y_test_fold)\n",
    "    print(f\"Fold {fold} - Test Loss: {loss}\")\n",
    "    \n",
    "    # Predictions and metrics\n",
    "    y_pred = vae_lstm.predict(x_test_fold)\n",
    "    mse = mean_squared_error(y_test_fold, y_pred)\n",
    "    mae = mean_absolute_error(y_test_fold, y_pred)\n",
    "    r2 = r2_score(y_test_fold, y_pred)\n",
    "    \n",
    "    print(f\"Fold {fold} - Mean Squared Error (MSE):\", mse)\n",
    "    print(f\"Fold {fold} - Mean Absolute Error (MAE):\", mae)\n",
    "    print(f\"Fold {fold} - R-squared (R2) Score:\", r2)\n",
    "    \n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_lstm.save('vae_lstm.h5')\n",
    "# from tensorflow.keras.models import load_model\n",
    "# vae_lstm = load_model('vae_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7769e",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5725e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "feature=1\n",
    "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],feature))\n",
    "print(x_train.shape)\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values\n",
    "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c242d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6,1)))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(units=150, activation='relu'))\n",
    "lstm.add(Dense(units=1))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "lstm.fit(x_train, y_train, epochs=10, batch_size=8)\n",
    "loss = lstm.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = lstm.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('lstm.h5')\n",
    "# from tensorflow.keras.models import load_model\n",
    "# lstm = load_model('lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfa653-1776-4a04-95ab-f5361946b6aa",
   "metadata": {},
   "source": [
    "# Theoharidis, et. al., 2023 (VAE-ConvLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9424da-c8e4-4387-8bff-092a68654e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "input_shape = (6, 1)\n",
    "latent_dim = 16\n",
    "\n",
    "def build_vae(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # VAE Encoder\n",
    "    x = Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    # Sampling function\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # VAE Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(np.prod(input_shape))(latent_inputs)\n",
    "    x = Reshape(input_shape)(x)\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    outputs = Conv1D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    outputs = decoder(z)\n",
    "\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    # VAE Loss Calculation\n",
    "    def vae_loss(inputs, outputs):\n",
    "        reconstruction_loss = K.mean(K.square(inputs - outputs))\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "        return reconstruction_loss + kl_loss\n",
    "\n",
    "    vae.add_loss(vae_loss(inputs, outputs))\n",
    "    vae.compile(optimizer='adam')\n",
    "\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "encoder, decoder, vae = build_vae(input_shape, latent_dim)\n",
    "vae.summary()\n",
    "\n",
    "X = df_reduced.values.reshape((df_reduced.shape[0], 6, 1))\n",
    "y = cleaned_df_normalized_df['Patv'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_scores, mae_scores, r2_scores = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    vae.fit(X_train, X_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    X_train_encoded = encoder.predict(X_train)[2]\n",
    "    X_test_encoded = encoder.predict(X_test)[2]\n",
    "\n",
    "    conv_input = Input(shape=(X_train_encoded.shape[1],))\n",
    "    dense = Dense(64, activation='relu')(conv_input)\n",
    "    output = Dense(1)(dense)\n",
    "    conv_model = Model(conv_input, output)\n",
    "    conv_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    conv_model.fit(X_train_encoded, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    predictions = conv_model.predict(X_test_encoded)\n",
    "    mse_scores.append(mean_squared_error(y_test, predictions))\n",
    "    mae_scores.append(mean_absolute_error(y_test, predictions))\n",
    "    r2_scores.append(r2_score(y_test, predictions))\n",
    "\n",
    "print(\"Mean MSE across folds:\", np.mean(mse_scores))\n",
    "print(\"Mean MAE across folds:\", np.mean(mae_scores))\n",
    "print(\"Mean R2 across folds:\", np.mean(r2_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc36c0a-b12e-476e-a85d-0b5a235ea305",
   "metadata": {},
   "source": [
    "# Kaur, et. al., 2023 (Bayesian BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad47e8-9460-4df5-8f00-6e3a19a2dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, Flatten, Reshape, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "input_shape = (6, 1)\n",
    "latent_dim = 2\n",
    "\n",
    "# VAE model definition\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_vae(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(np.prod(input_shape), activation='relu')(latent_inputs)\n",
    "    x = Reshape(input_shape)(x)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    # Loss calculation\n",
    "    reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= np.prod(input_shape)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "\n",
    "    vae.compile(optimizer=Adam())\n",
    "\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "# Build the VAE model\n",
    "encoder, decoder, vae = build_vae(input_shape, latent_dim)\n",
    "\n",
    "# Prepare data for training\n",
    "X = df_reduced.values.reshape((df_reduced.shape[0], 6, 1))\n",
    "y = cleaned_df_normalized_df['Patv'].values\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train VAE\n",
    "    vae.fit(X_train, X_train, epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Encode training data and test data\n",
    "    z_train, _, _ = encoder.predict(X_train)\n",
    "    z_test, _, _ = encoder.predict(X_test)\n",
    "    \n",
    "    # Bayesian BiLSTM model\n",
    "    def build_bayesian_bilstm(input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "        x = Bidirectional(LSTM(32))(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    bilstm = build_bayesian_bilstm((latent_dim, 1))\n",
    "    \n",
    "    # Reshape latent vectors for BiLSTM input\n",
    "    z_train_bilstm = z_train.reshape(z_train.shape[0], latent_dim, 1)\n",
    "    z_test_bilstm = z_test.reshape(z_test.shape[0], latent_dim, 1)\n",
    "    \n",
    "    # Train BiLSTM\n",
    "    bilstm.fit(z_train_bilstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = bilstm.predict(z_test_bilstm)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    print(f\"Fold - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Mean evaluation metrics\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}, Average MSE: {np.mean(mse_scores)}, Average R2: {np.mean(r2_scores)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0d174-df33-414b-b205-1d4893e1404e",
   "metadata": {},
   "source": [
    "# Harror, et. al., 2022 (GAHD-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde30b79-1d21-4dd3-b90e-61be9058e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, Flatten, Reshape, Bidirectional, LSTM, Attention, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "input_shape = (6, 1)\n",
    "latent_dim = 2\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def attention_block(inputs):\n",
    "    attention = Attention()([inputs, inputs])\n",
    "    return Multiply()([inputs, attention])\n",
    "\n",
    "def build_gahd_vae(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder with attention\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = attention_block(x)\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # Decoder with BiLSTM\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(np.prod(input_shape), activation='relu')(latent_inputs)\n",
    "    x = Reshape(input_shape)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = attention_block(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= np.prod(input_shape)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=Adam())\n",
    "\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "encoder, decoder, vae = build_gahd_vae(input_shape, latent_dim)\n",
    "\n",
    "X = df_reduced.values.reshape((df_reduced.shape[0], 6, 1))\n",
    "y = cleaned_df_normalized_df['Patv'].values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    vae.fit(X_train, X_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    z_train, _, _ = encoder.predict(X_train)\n",
    "    z_test, _, _ = encoder.predict(X_test)\n",
    "\n",
    "    def build_bayesian_bilstm(input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "        x = Bidirectional(LSTM(32))(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    bilstm = build_bayesian_bilstm((latent_dim, 1))\n",
    "\n",
    "    z_train_bilstm = z_train.reshape(z_train.shape[0], latent_dim, 1)\n",
    "    z_test_bilstm = z_test.reshape(z_test.shape[0], latent_dim, 1)\n",
    "\n",
    "    bilstm.fit(z_train_bilstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    y_pred = bilstm.predict(z_test_bilstm)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    print(f\"Fold - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}, Average MSE: {np.mean(mse_scores)}, Average R2: {np.mean(r2_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c406f-8ddd-4eee-ab2f-7d711a2cb965",
   "metadata": {},
   "source": [
    "# Atashfaraz, et. al., 2022 (V-LSTM-MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd6bb2-eb32-4323-a68c-a52830eb2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, Flatten, Reshape, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "input_shape = (6, 1)\n",
    "latent_dim = 2\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_v_lstm_mlp(input_shape, latent_dim):\n",
    "    # Encoder (VAE)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    # Decoder (LSTM-MLP)\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(np.prod(input_shape), activation='relu')(latent_inputs)\n",
    "    x = Reshape(input_shape)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # VAE Model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='v_lstm_mlp')\n",
    "\n",
    "    # Loss Calculation\n",
    "    reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= np.prod(input_shape)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=Adam())\n",
    "\n",
    "    return encoder, decoder, vae\n",
    "\n",
    "# Build model\n",
    "encoder, decoder, vae = build_v_lstm_mlp(input_shape, latent_dim)\n",
    "\n",
    "# Prepare data for training\n",
    "X = df_reduced.values.reshape((df_reduced.shape[0], 6, 1))\n",
    "y = cleaned_df_normalized_df['Patv'].values\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train V-LSTM-MLP\n",
    "    vae.fit(X_train, X_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    # Encode training data and test data\n",
    "    z_train, _, _ = encoder.predict(X_train)\n",
    "    z_test, _, _ = encoder.predict(X_test)\n",
    "\n",
    "    # Reshape latent vectors for decoder\n",
    "    y_pred = decoder.predict(z_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    print(f\"Fold - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Mean evaluation metrics\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}, Average MSE: {np.mean(mse_scores)}, Average R2: {np.mean(r2_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f397762-7f13-4587-8cb4-29291bf6adf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
